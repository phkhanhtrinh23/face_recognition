{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Gradio - Face Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gFgnoW3F5uA"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNXXnN5DPqOK",
        "outputId": "39a2bb86-40c1-4828-9a81-5493ef36162e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofW6jC-_P484",
        "outputId": "6634f7bb-917e-43cf-fbc7-481754f6f1ca"
      },
      "source": [
        "%cd drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVxgL8a5Pci5"
      },
      "source": [
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import joblib\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVppnEbzj8vs"
      },
      "source": [
        "dict = {\n",
        "    0: 'Dương',\n",
        "    1: 'Hiếu',\n",
        "    2: 'Hùng',\n",
        "    3: 'HĐức',\n",
        "    4: 'Khôi',\n",
        "    5: 'Kiên',\n",
        "    6: 'Linh',\n",
        "    7: 'Quân',\n",
        "    8: 'Tâm',\n",
        "    9: 'Tân',\n",
        "    10: 'Thắng',\n",
        "    11: 'Trình',\n",
        "    12: 'Trường',\n",
        "    13: 'Tuấn',\n",
        "    14: 'Vân',\n",
        "    15: 'Việt Đức',\n",
        "    16: 'Xuân Anh',\n",
        "    17: 'Đức'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tChSc7OgYPm-"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJUJLWQ92g6R",
        "outputId": "89f7bfcf-1c9d-49c4-c835-ba47a00e5360"
      },
      "source": [
        "!pip install -q gradio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 979 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 206 kB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 33.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 961 kB 18.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 35.7 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e200MmBU2aLT",
        "outputId": "35944917-63c7-475f-9d10-39301925685e"
      },
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "detector = dlib.cnn_face_detection_model_v1('./Save_model/face_recognition/mmod_human_face_detector.dat')\n",
        "\n",
        "def convert_and_trim_bb(image, rect):\n",
        "  startX = rect.left()\n",
        "  startY = rect.top()\n",
        "  endX = rect.right()\n",
        "  endY = rect.bottom()\n",
        "\n",
        "  startX = max(0, startX)\n",
        "  startY = max(0, startY)\n",
        "  endX = min(endX, image.shape[1])\n",
        "  endY = min(endY, image.shape[0])\n",
        "\n",
        "  w = endX - startX\n",
        "  h = endY - startY\n",
        "  \n",
        "  # return our bounding box coordinates\n",
        "  return (startX, startY, w, h)\n",
        "\n",
        "def predict(input_img):\n",
        "  image_path = []\n",
        "  image_path.append(input_img)\n",
        "\n",
        "  new_image_list = []\n",
        "  for image in image_path:\n",
        "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = detector(rgb, 1)\n",
        "    boxes = [convert_and_trim_bb(image, r.rect) for r in results]\n",
        "    crop_image = image\n",
        "\n",
        "    for (x, y, w, h) in boxes:\n",
        "      crop_image = image[y:y+h, x:x+w]\n",
        "  \n",
        "    new_image_list.append(crop_image)\n",
        "\n",
        "  image_list = []\n",
        "  for image in new_image_list:\n",
        "      resized_image = cv2.resize(image, (160,160), interpolation = cv2.INTER_AREA)\n",
        "      resized_image = np.expand_dims(resized_image, 0)\n",
        "      image_list.append(resized_image)\n",
        "      \n",
        "  image_list = np.vstack(image_list)\n",
        "\n",
        "  norm_image_list = []\n",
        "  for image in image_list:\n",
        "    image = image.astype(\"float32\")\n",
        "    mean, std = image.mean(), image.std()\n",
        "    image = (image - mean) / std\n",
        "\n",
        "    norm_image_list.append(image)\n",
        "\n",
        "  norm_image_list = np.asarray(norm_image_list)\n",
        "\n",
        "  facenet = load_model('./Save_model/face_recognition/facenet_keras.h5')\n",
        "\n",
        "  features = facenet.predict(norm_image_list)\n",
        "\n",
        "  normalizer = Normalizer(norm='l2')\n",
        "  features = normalizer.transform(features)\n",
        "\n",
        "  random_forest = joblib.load(\"./Save_model/face_recognition/random_forest.joblib\")\n",
        "\n",
        "  predictions = random_forest.predict_proba(features)\n",
        "\n",
        "  if predictions[0].max() < 0.65:\n",
        "    return \"This person is unknown.\"\n",
        "  else:\n",
        "    return \"This is: {} ({:.2f}%)\".format(dict[np.argmax(predictions[0])], predictions[0].max()*100)\n",
        "\n",
        "iface = gr.Interface(predict, inputs=gr.inputs.Image(shape=(1920, 1080)), outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://19866.gradio.app\n",
            "\n",
            "This share link will expire in 72 hours. To get longer links, send an email to: support@gradio.app\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://19866.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fbcf0ebe950>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbcf8292c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbcf1886710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc5e9771033f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0miface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1920\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1080\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gradio/interface.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, auth, auth_message, private_endpoint, prevent_thread_lock, show_error)\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         is_in_interactive_mode = bool(\n\u001b[1;32m    648\u001b[0m             getattr(sys, 'ps1', sys.flags.interactive))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}